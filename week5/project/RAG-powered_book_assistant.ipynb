{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8508f8e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')), '(Request ID: 050c4d81-7164-43c1-ab98-043e012ceaed)')' thrown while requesting HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/./modules.json\n",
      "Retrying in 1s [Retry 1/5].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model...\n",
      "Initializing LLM...\n",
      "\n",
      "==================================================\n",
      "Loading PDF and creating vector store...\n",
      "==================================================\n",
      "Loading existing vector store from vector_db...\n",
      "\n",
      "==================================================\n",
      "Asking Questions\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "Q: What is the first chapter about?\n",
      "==================================================\n",
      "\n",
      "A: The first chapter is about Happiness Is a Problem. This can be inferred from the title of Chapter 2, which is \"Happiness Is a Problem\", and the content of Excerpt 1, which mentions that this book will teach readers to lose and let go, rather than trying to achieve happiness or greatness.\n",
      "\n",
      "ðŸ“š Sources used (4 excerpts):\n",
      "\n",
      "  [1] Chunk 43: This book doesnâ€™t give a fuck about alleviating your problems or your pain. And that is precisely why you will know itâ€™s being honest. This book is no...\n",
      "\n",
      "  [2] Chunk 44: This book will not teach you how to gain or achieve, but rather how to lose and let go. It will teach you to take inventory of your life and scrub out...\n",
      "\n",
      "  [3] Chunk 335: Boundaries Once upon a time, there were two youngsters, a boy and a girl. Their families hated each other. But the boy snuck into a party hosted by th...\n",
      "\n",
      "  [4] Chunk 4: Upon signing the contract, Bukowski wrote his first novel in three weeks. It was called simply Post Office. In the dedication, he wrote, â€œDedicated to...\n",
      "\n",
      "==================================================\n",
      "Q: Who is the author?\n",
      "==================================================\n",
      "\n",
      "A: I don't know - this information is not in the provided excerpts.\n",
      "\n",
      "ðŸ“š Sources used (4 excerpts):\n",
      "\n",
      "  [1] Chunk 413: To Philip Kemper and Drew Birnieâ€”two big brains that conspire to make my brain appear much larger than it actually is. Your hard work and brilliance c...\n",
      "\n",
      "  [2] Chunk 43: This book doesnâ€™t give a fuck about alleviating your problems or your pain. And that is precisely why you will know itâ€™s being honest. This book is no...\n",
      "\n",
      "  [3] Chunk 4: Upon signing the contract, Bukowski wrote his first novel in three weeks. It was called simply Post Office. In the dedication, he wrote, â€œDedicated to...\n",
      "\n",
      "  [4] Chunk 382: to\n",
      "teach psychology, psychology\n",
      "textbooks\n",
      "to\n",
      "Becker then landed at San Francisco State University, where he actually kept his job for more than a year...\n",
      "\n",
      "==================================================\n",
      "Q: What did the author want to convey in second chapter?\n",
      "==================================================\n",
      "\n",
      "A: Based on the provided excerpt, I don't know what the author wants to convey in the second chapter. This information is not in the provided excerpts.\n",
      "\n",
      "ðŸ“š Sources used (4 excerpts):\n",
      "\n",
      "  [1] Chunk 411: serious look on his face.\n",
      "I brush my hands off on my shorts, my body still buzzing from my\n",
      "surrender. Thereâ€™s an awkward silence.\n",
      "The Aussie stands fo...\n",
      "\n",
      "  [2] Chunk 413: To Philip Kemper and Drew Birnieâ€”two big brains that conspire to make my brain appear much larger than it actually is. Your hard work and brilliance c...\n",
      "\n",
      "  [3] Chunk 43: This book doesnâ€™t give a fuck about alleviating your problems or your pain. And that is precisely why you will know itâ€™s being honest. This book is no...\n",
      "\n",
      "  [4] Chunk 44: This book will not teach you how to gain or achieve, but rather how to lose and let go. It will teach you to take inventory of your life and scrub out...\n",
      "\n",
      "==================================================\n",
      "Q: What is the name of the Second Lieutenant of Japanese Imperial Army?\n",
      "==================================================\n",
      "\n",
      "A: The name of the Second Lieutenant of Japanese Imperial Army is Hiroo Onoda. This information can be found in Excerpt 3, where it states: \"Second Lieutenant Hiroo Onoda of the Japanese Imperial Army was deployed to the small island of Lubang in the Philippines.\"\n",
      "\n",
      "ðŸ“š Sources used (4 excerpts):\n",
      "\n",
      "  [1] Chunk 127: Onoda, having now spent more than half of his life in the jungles of\n",
      "Lubang, was all alone.\n",
      "In 1972, the news of Kozukaâ€™s death reached Japan and caus...\n",
      "\n",
      "  [2] Chunk 126: But these, too, were ignored. In 1952, the Japanese government made one final effort to draw the last remaining soldiers out of hiding throughout the ...\n",
      "\n",
      "  [3] Chunk 122: Sounds boring, doesnâ€™t it? Thatâ€™s because these things are ordinary. But maybe theyâ€™re ordinary for a reason: because they are what actually matters.\n",
      "...\n",
      "\n",
      "  [4] Chunk 130: He found Onoda in four days. Suzuki stayed with Onoda in the jungle for some time. Onoda had been alone by that point for over a year, and once found ...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from typing import List, Optional\n",
    "import gradio as gr\n",
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from unstructured.partition.auto import partition\n",
    "from langchain_ollama import ChatOllama\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# Assuming you have this module\n",
    "sys.path.append(\"../../../llm_engineering\")\n",
    "from api_clients import create_clients\n",
    "\n",
    "\n",
    "class RAGBookQA:\n",
    "    \"\"\"RAG-based Question Answering system for PDF books.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        db_name: str = \"vector_db\",\n",
    "        embedding_model: str = \"all-MiniLM-L6-v2\",\n",
    "        chunk_size: int = 1000,\n",
    "        chunk_overlap: int = 200,\n",
    "        temperature: float = 0.0,\n",
    "        k_docs: int = 4\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the RAG system.\n",
    "        \n",
    "        Args:\n",
    "            db_name: Directory name for vector database\n",
    "            embedding_model: HuggingFace model name for embeddings\n",
    "            chunk_size: Size of text chunks for splitting\n",
    "            chunk_overlap: Overlap between chunks\n",
    "            temperature: LLM temperature for response generation\n",
    "            k_docs: Number of documents to retrieve\n",
    "        \"\"\"\n",
    "        self.db_name = db_name\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "        self.temperature = temperature\n",
    "        self.k_docs = k_docs\n",
    "        \n",
    "        # Initialize embeddings\n",
    "        print(\"Loading embedding model...\")\n",
    "        self.embeddings = HuggingFaceEmbeddings(model_name=embedding_model)\n",
    "        \n",
    "        # Initialize LLM\n",
    "        print(\"Initializing LLM...\")\n",
    "        clients = create_clients()\n",
    "        self.llm = ChatOllama(\n",
    "            model=clients[\"models\"][\"OLLAMA_MODEL\"],\n",
    "            temperature=self.temperature,\n",
    "        )\n",
    "        \n",
    "        self.vectorstore = None\n",
    "        self.retriever = None\n",
    "        \n",
    "    def load_and_process_pdf(self, filename: str, force_reload: bool = False) -> None:\n",
    "        \"\"\"\n",
    "        Load PDF, chunk it, and create vector store.\n",
    "        \n",
    "        Args:\n",
    "            filename: Path to PDF file\n",
    "            force_reload: If True, recreate vector store even if it exists\n",
    "        \"\"\"\n",
    "        if not os.path.exists(filename):\n",
    "            raise FileNotFoundError(f\"File not found: {filename}\")\n",
    "        \n",
    "        # Check if we need to reload\n",
    "        if os.path.exists(self.db_name) and not force_reload:\n",
    "            print(f\"Loading existing vector store from {self.db_name}...\")\n",
    "            self.vectorstore = Chroma(\n",
    "                persist_directory=self.db_name,\n",
    "                embedding_function=self.embeddings\n",
    "            )\n",
    "            self.retriever = self.vectorstore.as_retriever(\n",
    "                search_kwargs={\"k\": self.k_docs}\n",
    "            )\n",
    "            return\n",
    "        \n",
    "        # Delete existing collection if force reload\n",
    "        if os.path.exists(self.db_name):\n",
    "            print(\"Deleting existing vector store...\")\n",
    "            try:\n",
    "                Chroma(\n",
    "                    persist_directory=self.db_name,\n",
    "                    embedding_function=self.embeddings\n",
    "                ).delete_collection()\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not delete existing collection: {e}\")\n",
    "        \n",
    "        # Partition PDF\n",
    "        print(f\"Processing PDF: {filename}\")\n",
    "        try:\n",
    "            file_partition = partition(filename)\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to partition PDF: {e}\")\n",
    "        \n",
    "        # Combine text with metadata\n",
    "        text = '\\n'.join([str(el) for el in file_partition])\n",
    "        doc = Document(\n",
    "            page_content=text,\n",
    "            metadata={\n",
    "                \"source\": filename,\n",
    "                \"filename\": os.path.basename(filename)\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # Split into chunks (using RecursiveCharacterTextSplitter for better results)\n",
    "        print(\"Splitting document into chunks...\")\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=self.chunk_size,\n",
    "            chunk_overlap=self.chunk_overlap,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"] # it has hierarchical order\n",
    "        )\n",
    "        chunks = text_splitter.split_documents([doc])\n",
    "        \n",
    "        # Add chunk numbers to metadata\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            chunk.metadata[\"chunk_id\"] = i\n",
    "            chunk.metadata[\"total_chunks\"] = len(chunks)\n",
    "        \n",
    "        print(f\"Created {len(chunks)} chunks\")\n",
    "        \n",
    "        # Create vector store\n",
    "        print(\"Creating vector store...\")\n",
    "        try:\n",
    "            self.vectorstore = Chroma.from_documents(\n",
    "                documents=chunks,\n",
    "                embedding=self.embeddings,\n",
    "                persist_directory=self.db_name\n",
    "            )\n",
    "            print(\"Vector store created successfully\")\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to create vector store: {e}\")\n",
    "        \n",
    "        # Create retriever with MMR for better diversity\n",
    "        self.retriever = self.vectorstore.as_retriever(\n",
    "            search_type=\"mmr\",  # Maximal Marginal Relevance\n",
    "            search_kwargs={\n",
    "                \"k\": self.k_docs,\n",
    "                \"fetch_k\": self.k_docs * 2  # Fetch more, then rerank\n",
    "            }\n",
    "        )\n",
    "    \n",
    "    def answer_question(\n",
    "        self,\n",
    "        question: str,\n",
    "        show_sources: bool = True,\n",
    "        verbose: bool = False\n",
    "    ) -> dict:\n",
    "        \"\"\"\n",
    "        Answer a question based on the loaded PDF.\n",
    "        \n",
    "        Args:\n",
    "            question: User's question\n",
    "            show_sources: Whether to include source information\n",
    "            verbose: Whether to print retrieved documents\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with answer and optional metadata\n",
    "        \"\"\"\n",
    "        if self.retriever is None:\n",
    "            return {\n",
    "                \"answer\": \"Error: No document loaded. Please load a PDF first.\",\n",
    "                \"sources\": []\n",
    "            }\n",
    "        \n",
    "        # Retrieve relevant documents\n",
    "        try:\n",
    "            docs = self.retriever.invoke(question)\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"answer\": f\"Error retrieving documents: {e}\",\n",
    "                \"sources\": []\n",
    "            }\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\nRetrieved {len(docs)} documents:\")\n",
    "            for i, doc in enumerate(docs):\n",
    "                print(f\"\\n--- Document {i+1} ---\")\n",
    "                print(doc.page_content[:200] + \"...\")\n",
    "        \n",
    "        if not docs:\n",
    "            return {\n",
    "                \"answer\": \"I don't know. No relevant context found in the document.\",\n",
    "                \"sources\": []\n",
    "            }\n",
    "        \n",
    "        # Prepare context\n",
    "        context = \"\\n\\n\".join([\n",
    "            f\"[Excerpt {i+1}]:\\n{doc.page_content}\"\n",
    "            for i, doc in enumerate(docs)\n",
    "        ])\n",
    "        \n",
    "        # Create prompt\n",
    "        prompt = f\"\"\"You are a helpful assistant that answers questions strictly based on the provided context from a book.\n",
    "                CONTEXT:\n",
    "                {context}\n",
    "\n",
    "                QUESTION:\n",
    "                {question}\n",
    "\n",
    "                INSTRUCTIONS:\n",
    "                - Answer the question using ONLY information from the context above\n",
    "                - Be specific and cite relevant details from the context\n",
    "                - If the answer cannot be found in the context, respond with: \"I don't know - this information is not in the provided excerpts.\"\n",
    "                - Do not make assumptions or add information not present in the context\n",
    "\n",
    "                ANSWER:\"\"\"\n",
    "        \n",
    "        # Get LLM response\n",
    "        try:\n",
    "            response = self.llm.invoke([HumanMessage(content=prompt)])\n",
    "            answer = response.content.strip()\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"answer\": f\"Error generating answer: {e}\",\n",
    "                \"sources\": []\n",
    "            }\n",
    "        \n",
    "        # Prepare result\n",
    "        result = {\"answer\": answer}\n",
    "        \n",
    "        if show_sources:\n",
    "            sources = []\n",
    "            for i, doc in enumerate(docs):\n",
    "                sources.append({\n",
    "                    \"chunk_id\": doc.metadata.get(\"chunk_id\", \"unknown\"),\n",
    "                    \"preview\": doc.page_content[:150] + \"...\",\n",
    "                    \"filename\": doc.metadata.get(\"filename\", \"unknown\")\n",
    "                })\n",
    "            result[\"sources\"] = sources\n",
    "        \n",
    "        return result\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Example usage of the RAG system.\"\"\"\n",
    "    \n",
    "    # Configuration\n",
    "    PDF_PATH = \"C:/Users/Paing/Downloads/fcuk.pdf\"\n",
    "    \n",
    "    # Initialize RAG system\n",
    "    rag_system = RAGBookQA(\n",
    "        db_name=\"vector_db\",\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=200,\n",
    "        k_docs=4\n",
    "    )\n",
    "    \n",
    "    # Load and process PDF (set force_reload=True to rebuild vector store)\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Loading PDF and creating vector store...\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    try:\n",
    "        rag_system.load_and_process_pdf(PDF_PATH, force_reload=False)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading PDF: {e}\")\n",
    "        return\n",
    "    \n",
    "    # Ask questions\n",
    "    questions = [\n",
    "        \"What is the first chapter about?\",\n",
    "        \"Who is the author?\",\n",
    "        \"What did the author want to convey in second chapter?\",\n",
    "        \"What is the name of the Second Lieutenant of Japanese Imperial Army?\"\n",
    "    ]\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Asking Questions\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    for question in questions:\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Q: {question}\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        result = rag_system.answer_question(\n",
    "            question,\n",
    "            show_sources=True,\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nA: {result['answer']}\")\n",
    "        \n",
    "        if result.get('sources'):\n",
    "            print(f\"\\nðŸ“š Sources used ({len(result['sources'])} excerpts):\")\n",
    "            for i, source in enumerate(result['sources']):\n",
    "                print(f\"\\n  [{i+1}] Chunk {source['chunk_id']}: {source['preview']}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632ed6f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
