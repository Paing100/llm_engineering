{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8508f8e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\paing\\anaconda3\\envs\\llms\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model...\n",
      "Initializing LLM...\n",
      "\n",
      "==================================================\n",
      "Loading PDF and creating vector store...\n",
      "==================================================\n",
      "Processing PDF: C:/Users/Paing/Downloads/fcuk.pdf\n",
      "Warning: No languages specified, defaulting to English.\n",
      "Splitting document into chunks...\n",
      "Created 417 chunks\n",
      "Creating vector store...\n",
      "Vector store created successfully\n",
      "Loading existing vector store from vector_db...\n",
      "\n",
      "==================================================\n",
      "Asking Questions\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "Q: What is the first chapter of subtle art of not giving a f about?\n",
      "==================================================\n",
      "\n",
      "A: I don't know - this information is not in the provided excerpts. The text does not explicitly mention a chapter title, and the excerpts appear to be an introduction to the concept of \"not giving a f about\" rather than a structured book with chapters.\n",
      "\n",
      "ðŸ“š Sources used (4 excerpts):\n",
      "\n",
      "  [1] Chunk 30: Thereâ€™s a name for a person who finds no emotion or meaning in anything: a psychopath. Why you would want to emulate a psychopath, I have no fucking c...\n",
      "\n",
      "  [2] Chunk 38: It then follows that finding something important and meaningful in your life is perhaps the most productive use of your time and energy. Because if\n",
      "yo...\n",
      "\n",
      "  [3] Chunk 31: So what does not giving a fuck mean? Letâ€™s look at three â€œsubtletiesâ€\n",
      "that should help clarify the matter.\n",
      "Subtlety #1: Not giving a fuck does not mea...\n",
      "\n",
      "  [4] Chunk 28: There is a subtle art to not giving a fuck. And though the concept may sound ridiculous and I may sound like an asshole, what Iâ€™m talking about here i...\n",
      "\n",
      "==================================================\n",
      "Q: Who is the author of 1984?\n",
      "==================================================\n",
      "\n",
      "A: I don't know - this information is not in the provided excerpts. The context mentions authors such as Charles Bukowski, Becker (likely Albert Becker), and Mark Manson, but does not mention 1984 or its author, George Orwell.\n",
      "\n",
      "ðŸ“š Sources used (4 excerpts):\n",
      "\n",
      "  [1] Chunk 413: To Philip Kemper and Drew Birnieâ€”two big brains that conspire to make my brain appear much larger than it actually is. Your hard work and brilliance c...\n",
      "\n",
      "  [2] Chunk 4: Upon signing the contract, Bukowski wrote his first novel in three weeks. It was called simply Post Office. In the dedication, he wrote, â€œDedicated to...\n",
      "\n",
      "  [3] Chunk 382: to\n",
      "teach psychology, psychology\n",
      "textbooks\n",
      "to\n",
      "Becker then landed at San Francisco State University, where he actually kept his job for more than a year...\n",
      "\n",
      "  [4] Chunk 414: And finally, to the millions of people who, for whatever reason, decided to read a potty-mouthed asshole from Boston writing about life on his blog. T...\n",
      "\n",
      "==================================================\n",
      "Q: What is the name of the Second Lieutenant of Japanese Imperial Army in subtle art of not giving a f?\n",
      "==================================================\n",
      "\n",
      "A: The name of the Second Lieutenant of Japanese Imperial Army mentioned in the context is Hiroo Onoda.\n",
      "\n",
      "ðŸ“š Sources used (4 excerpts):\n",
      "\n",
      "  [1] Chunk 126: But these, too, were ignored. In 1952, the Japanese government made one final effort to draw the last remaining soldiers out of hiding throughout the ...\n",
      "\n",
      "  [2] Chunk 127: Onoda, having now spent more than half of his life in the jungles of\n",
      "Lubang, was all alone.\n",
      "In 1972, the news of Kozukaâ€™s death reached Japan and caus...\n",
      "\n",
      "  [3] Chunk 122: Sounds boring, doesnâ€™t it? Thatâ€™s because these things are ordinary. But maybe theyâ€™re ordinary for a reason: because they are what actually matters.\n",
      "...\n",
      "\n",
      "  [4] Chunk 124: However, thousands of Japanese soldiers were still scattered among the Pacific isles, and most, like Onoda, were hiding in the jungle, unaware that th...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from typing import List, Optional\n",
    "import gradio as gr\n",
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from unstructured.partition.auto import partition\n",
    "from langchain_ollama import ChatOllama\n",
    "from IPython.display import Markdown, display\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Assuming you have this module\n",
    "sys.path.append(\"../../../llm_engineering\")\n",
    "from api_clients import create_clients\n",
    "\n",
    "\n",
    "class RAGBookQA:\n",
    "    \"\"\"RAG-based Question Answering system for PDF books.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        db_name: str = \"vector_db\",\n",
    "        embedding_model: str = \"all-MiniLM-L6-v2\",\n",
    "        chunk_size: int = 1000,\n",
    "        chunk_overlap: int = 200,\n",
    "        temperature: float = 0.0,\n",
    "        k_docs: int = 4\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the RAG system.\n",
    "        \n",
    "        Args:\n",
    "            db_name: Directory name for vector database\n",
    "            embedding_model: HuggingFace model name for embeddings\n",
    "            chunk_size: Size of text chunks for splitting\n",
    "            chunk_overlap: Overlap between chunks\n",
    "            temperature: LLM temperature for response generation\n",
    "            k_docs: Number of documents to retrieve\n",
    "        \"\"\"\n",
    "        self.db_name = db_name\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "        self.temperature = temperature\n",
    "        self.k_docs = k_docs\n",
    "        \n",
    "        # Initialize embeddings\n",
    "        print(\"Loading embedding model...\")\n",
    "        self.embeddings = HuggingFaceEmbeddings(model_name=embedding_model)\n",
    "        \n",
    "        # Initialize LLM\n",
    "        print(\"Initializing LLM...\")\n",
    "        clients = create_clients()\n",
    "        self.llm = ChatOllama(\n",
    "            model=clients[\"models\"][\"OLLAMA_MODEL\"],\n",
    "            temperature=self.temperature,\n",
    "        )\n",
    "        \n",
    "        self.vectorstore = None\n",
    "        self.retriever = None\n",
    "        \n",
    "    def load_and_process_pdf(self, filename: str, force_reload: bool = False) -> None:\n",
    "        \"\"\"\n",
    "        Load PDF, chunk it, and create vector store.\n",
    "        \n",
    "        Args:\n",
    "            filename: Path to PDF file\n",
    "            force_reload: If True, recreate vector store even if it exists\n",
    "        \"\"\"\n",
    "        if not os.path.exists(filename):\n",
    "            raise FileNotFoundError(f\"File not found: {filename}\")\n",
    "        \n",
    "        # Check if we need to reload\n",
    "        if os.path.exists(self.db_name) and not force_reload:\n",
    "            print(f\"Loading existing vector store from {self.db_name}...\")\n",
    "            self.vectorstore = Chroma(\n",
    "                persist_directory=self.db_name,\n",
    "                embedding_function=self.embeddings\n",
    "            )\n",
    "\n",
    "            self.retriever = self.vectorstore.as_retriever(\n",
    "                search_kwargs={\"k\": self.k_docs}\n",
    "            )\n",
    "            return\n",
    "        \n",
    "        # Delete existing collection if force reload\n",
    "        if os.path.exists(self.db_name):\n",
    "            print(\"Deleting existing vector store...\")\n",
    "            try:\n",
    "                Chroma(\n",
    "                    persist_directory=self.db_name,\n",
    "                    embedding_function=self.embeddings\n",
    "                ).delete_collection()\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not delete existing collection: {e}\")\n",
    "        \n",
    "        # Partition PDF\n",
    "        print(f\"Processing PDF: {filename}\")\n",
    "        try:\n",
    "            file_partition = partition(filename)\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to partition PDF: {e}\")\n",
    "        \n",
    "        # Combine text with metadata\n",
    "        text = '\\n'.join([str(el) for el in file_partition])\n",
    "        doc = Document(\n",
    "            page_content=text,\n",
    "            metadata={\n",
    "                \"source\": filename,\n",
    "                \"filename\": os.path.basename(filename)\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # Split into chunks (using RecursiveCharacterTextSplitter for better results)\n",
    "        print(\"Splitting document into chunks...\")\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=self.chunk_size,\n",
    "            chunk_overlap=self.chunk_overlap,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"] # it has hierarchical order\n",
    "        )\n",
    "        chunks = text_splitter.split_documents([doc])\n",
    "        \n",
    "        # Add chunk numbers to metadata\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            chunk.metadata[\"chunk_id\"] = i\n",
    "            chunk.metadata[\"total_chunks\"] = len(chunks)\n",
    "            chunk.metadata[\"file_name\"] = filename\n",
    "        \n",
    "        print(f\"Created {len(chunks)} chunks\")\n",
    "        \n",
    "        # Create vector store\n",
    "        print(\"Creating vector store...\")\n",
    "        try:\n",
    "            self.vectorstore = Chroma.from_documents(\n",
    "                documents=chunks,\n",
    "                embedding=self.embeddings,\n",
    "                persist_directory=self.db_name\n",
    "            )\n",
    "            print(\"Vector store created successfully\")\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to create vector store: {e}\")\n",
    "        \n",
    "        # Create retriever with MMR for better diversity\n",
    "        self.retriever = self.vectorstore.as_retriever(\n",
    "            search_type=\"mmr\",  # Maximal Marginal Relevance\n",
    "            search_kwargs={\n",
    "                \"k\": self.k_docs,\n",
    "                \"fetch_k\": self.k_docs * 2  # Fetch more, then rerank\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # result = self.vectorstore._collection.get(include=['embeddings', 'documents', 'metadatas'])\n",
    "        # vectors = np.array(result['embeddings'])\n",
    "        # documents = result['documents']\n",
    "        # doc_types = [metadata['file_name'] for metadata in result['metadatas']]\n",
    "        # colors = [['blue', 'green', 'red', 'orange'][['products', 'employees', 'contracts', 'company'].index(t)] for t in doc_types]\n",
    "        # tsne = TSNE(n_components=2, random_state=42)\n",
    "        # reduced_vectors = tsne.fit_transform(vectors)\n",
    "\n",
    "        # # Create the 2D scatter plot\n",
    "        # fig = go.Figure(data=[go.Scatter(\n",
    "        #     x=reduced_vectors[:, 0],\n",
    "        #     y=reduced_vectors[:, 1],\n",
    "        #     mode='markers',\n",
    "        #     marker=dict(size=5, color=colors, opacity=0.8),\n",
    "        #     text=[f\"Type: {t}<br>Text: {d[:100]}...\" for t, d in zip(doc_types, documents)],\n",
    "        #     hoverinfo='text'\n",
    "        # )])\n",
    "\n",
    "        # fig.update_layout(\n",
    "        #     title='2D Chroma Vector Store Visualization',\n",
    "        #     scene=dict(xaxis_title='x',yaxis_title='y'),\n",
    "        #     width=800,\n",
    "        #     height=600,\n",
    "        #     margin=dict(r=20, b=10, l=10, t=40)\n",
    "        # )\n",
    "\n",
    "        # fig.show()\n",
    "    \n",
    "    def answer_question(\n",
    "        self,\n",
    "        question: str,\n",
    "        show_sources: bool = True,\n",
    "        verbose: bool = False\n",
    "    ) -> dict:\n",
    "        \"\"\"\n",
    "        Answer a question based on the loaded PDF.\n",
    "        \n",
    "        Args:\n",
    "            question: User's question\n",
    "            show_sources: Whether to include source information\n",
    "            verbose: Whether to print retrieved documents\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with answer and optional metadata\n",
    "        \"\"\"\n",
    "        if self.retriever is None:\n",
    "            return {\n",
    "                \"answer\": \"Error: No document loaded. Please load a PDF first.\",\n",
    "                \"sources\": []\n",
    "            }\n",
    "        \n",
    "        # Retrieve relevant documents\n",
    "        try:\n",
    "            docs = self.retriever.invoke(question)\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"answer\": f\"Error retrieving documents: {e}\",\n",
    "                \"sources\": []\n",
    "            }\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\nRetrieved {len(docs)} documents:\")\n",
    "            for i, doc in enumerate(docs):\n",
    "                print(f\"\\n--- Document {i+1} ---\")\n",
    "                print(doc.page_content[:200] + \"...\")\n",
    "        \n",
    "        if not docs:\n",
    "            return {\n",
    "                \"answer\": \"I don't know. No relevant context found in the document.\",\n",
    "                \"sources\": []\n",
    "            }\n",
    "        \n",
    "        # Prepare context\n",
    "        context = \"\\n\\n\".join([\n",
    "            f\"[Excerpt {i+1}]:\\n{doc.page_content}\"\n",
    "            for i, doc in enumerate(docs)\n",
    "        ])\n",
    "        \n",
    "        # Create prompt\n",
    "        prompt = f\"\"\"You are a helpful assistant that answers questions strictly based on the provided context from a book.\n",
    "                CONTEXT:\n",
    "                {context}\n",
    "\n",
    "                QUESTION:\n",
    "                {question}\n",
    "\n",
    "                INSTRUCTIONS:\n",
    "                - Answer the question using ONLY information from the context above\n",
    "                - Be specific and cite relevant details from the context\n",
    "                - If the answer cannot be found in the context, respond with: \"I don't know - this information is not in the provided excerpts.\"\n",
    "                - Do not make assumptions or add information not present in the context\n",
    "\n",
    "                ANSWER:\"\"\"\n",
    "        \n",
    "        # Get LLM response\n",
    "        try:\n",
    "            response = self.llm.invoke([HumanMessage(content=prompt)])\n",
    "            answer = response.content.strip()\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"answer\": f\"Error generating answer: {e}\",\n",
    "                \"sources\": []\n",
    "            }\n",
    "        \n",
    "        # Prepare result\n",
    "        result = {\"answer\": answer}\n",
    "        \n",
    "        if show_sources:\n",
    "            sources = []\n",
    "            for i, doc in enumerate(docs):\n",
    "                sources.append({\n",
    "                    \"chunk_id\": doc.metadata.get(\"chunk_id\", \"unknown\"),\n",
    "                    \"preview\": doc.page_content[:150] + \"...\",\n",
    "                    \"filename\": doc.metadata.get(\"filename\", \"unknown\")\n",
    "                })\n",
    "            result[\"sources\"] = sources\n",
    "        \n",
    "        return result\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Example usage of the RAG system.\"\"\"\n",
    "    \n",
    "    # Configuration\n",
    "    subtle_art = \"C:/Users/Paing/Downloads/fcuk.pdf\"\n",
    "    g_owell = \"C:/Users/Paing/Downloads/1984.pdf\"\n",
    "\n",
    "    # Initialize RAG system\n",
    "    rag_system = RAGBookQA(\n",
    "        db_name=\"vector_db\",\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=200,\n",
    "        k_docs=4\n",
    "    )\n",
    "    \n",
    "    # Load and process PDF (set force_reload=True to rebuild vector store)\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Loading PDF and creating vector store...\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    try:\n",
    "        rag_system.load_and_process_pdf(subtle_art, force_reload=False)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading PDF: {e}\")\n",
    "        return\n",
    "    \n",
    "    # Ask questions\n",
    "    questions = [\n",
    "        \"What is the first chapter of subtle art of not giving a f about?\",\n",
    "        \"Who is the author of 1984?\",\n",
    "        \"What is the name of the Second Lieutenant of Japanese Imperial Army in subtle art of not giving a f?\"\n",
    "    ]\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Asking Questions\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    for question in questions:\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Q: {question}\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        result = rag_system.answer_question(\n",
    "            question,\n",
    "            show_sources=True,\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nA: {result['answer']}\")\n",
    "        \n",
    "        if result.get('sources'):\n",
    "            print(f\"\\nðŸ“š Sources used ({len(result['sources'])} excerpts):\")\n",
    "            for i, source in enumerate(result['sources']):\n",
    "                print(f\"\\n  [{i+1}] Chunk {source['chunk_id']}: {source['preview']}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd08609",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
